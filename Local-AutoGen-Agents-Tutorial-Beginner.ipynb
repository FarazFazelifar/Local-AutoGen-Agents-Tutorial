{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Ollama](#toc1_)      \n",
    "- [Agents](#toc3_)    \n",
    "  - [UserProxyAgent](#toc3_1_)    \n",
    "  - [ConversableAgent](#toc3_2_)    \n",
    "  - [AssistantAgent](#toc3_3_)    \n",
    "  - [GroupChatManager](#toc3_4_)    \n",
    "  - [LocalCommandLineCodeExecutor](#toc3_5_)    \n",
    "    - [code_execution_config](#toc3_5_1_)    \n",
    "- [Implementing Custom Agents](#toc4_)    \n",
    "  - [RAG Agent](#toc4_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Ollama](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[](#toc0_)\n",
    "We want to use AutoGen alongside Ollama. To do this, we have to follow some steps first.\n",
    "Since AutoGent is integrated with the OpenAI API, we have to find a workaround to be able to use Ollama with it.\n",
    "A solution, is Launching our local LLM into a local API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was actually played at Globe Life Field in Arlington, Texas (hosting team Dallas Rangers) and other various AL and NL parks due to Covid-19 protocols allowing teams to have a home game designation. However the final 6 games were hosted at Globe life field. The Dodgers played the Tampa Bay Rays, but only had to play the minimum 4 games necessary for a series win in the World Series that year\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1', # Local ollama address\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3.1\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is working, we can start importing our agents and designing our system using AutoGen. First of all, we will need an llm config for each agent. we can customize them for each agent specigically, or use one config for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "config_list = [\n",
    "  {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"base_url\": \"http://localhost:11434/v1\",\n",
    "    \"api_key\": \"ollama\",\n",
    "  }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Agents](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[UserProxyAgent](#toc0_)\n",
    "The user proxy agent is one of the basic agents. It represents a human user in the conversation. In simple terms, this agent is responsible for carrying your commands to the rest of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import UserProxyAgent\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\", \n",
    "    max_consecutive_auto_reply=10, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[ConversableAgent](#toc0_)\n",
    "This is the simplest of agents. It is the base class for agents capable of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "conversable_agent = ConversableAgent(\n",
    "    name=\"conversable_agent\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "reply = conversable_agent.generate_reply(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, who are you?\"}],\n",
    "    sender=user_proxy\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[AssistantAgent](#toc0_)\n",
    "This AI Agent is one of the key agents in the system. it can assist with various tasks such as general QA, writing, problem solving, code generation, code explanation, data analysis, task planning, translation, maths, researching etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "reply = assistant.generate_reply(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, who are you?\"}],\n",
    "    sender=user_proxy\n",
    ")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[GroupChatManager](#toc0_)\n",
    "You can think of this agent as the manager of a work group. The manager, as the name suggests, manages the conversations between multiple agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChatManager, GroupChat\n",
    "\n",
    "# Create multiple agents\n",
    "agent1 = AssistantAgent(\"agent1\", llm_config=llm_config)\n",
    "agent2 = AssistantAgent(\"agent2\", llm_config=llm_config)\n",
    "\n",
    "# Create a GroupChatManager\n",
    "group_chat = GroupChat(\n",
    "    agents=[user_proxy, assistant, agent1, agent2],\n",
    "    messages=[],\n",
    "    max_round=12\n",
    ")\n",
    "\n",
    "# Create a GroupChatManager\n",
    "manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"Discuss the pros and cons of renewable energy.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[LocalCommandLineCodeExecutor](#toc0_)\n",
    "This agent is probably the most advanced of the agents thus far. Not only can it generate code, but it can also execute the code it writes, given the right environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_1_'></a>[code_execution_config](#toc0_)\n",
    "The code execution config defines the parameters for code execution, including which executor to use, where to execute the code, and any specific settings for the execution environment.\n",
    "\n",
    "The parameters for this config include:\n",
    "1- executor\n",
    "2- work_dir\n",
    "3- use_docker\n",
    "4- last_n_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "import os\n",
    "\n",
    "executor = LocalCommandLineCodeExecutor(\n",
    "    timeout=60,  # timeout in seconds\n",
    "    work_dir=\"coding\"  # working directory\n",
    ")\n",
    "\n",
    "code_execution_config = {\n",
    "    \"executor\": executor,\n",
    "    \"execution_policies\": {\n",
    "        \"python\": True,  # Allow execution of Python code\n",
    "        \"shell\": True  # False: Only save shell scripts, do not execute/ True: executes the script as well\n",
    "    },\n",
    "    \"last_n_messages\": 3,\n",
    "}\n",
    "\n",
    "code_assistant = ConversableAgent(\n",
    "    name=\"code_executor_agent\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=code_execution_config\n",
    ")\n",
    "\n",
    "response = user_proxy.initiate_chat(\n",
    "    code_assistant,\n",
    "    message=\"write a code to calculate the sin of x. store it in your working directory. run the code to make sure it is correct. if you encounter an error or the answer is wrong, write the code again.\"\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
